{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Hybrid cloud machine learning with Flux.jl\n",
    "\n",
    "This tutorial demonstrates how we can use AzureClusterlessHPC to develop a machine learning model on our local machine, while running the training on a remote machine with a GPU. This hybrid cloud scenarios enables researchers to make efficient use of expensive GPU instances without having to manually move data & code from and to GPU VMs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages for this example\n",
    "using Pkg; Pkg.add(\"Flux\")\n",
    "\n",
    "# Set paths to credentials + parameters\n",
    "ENV[\"CREDENTIALS\"] = joinpath(pwd(), \"../..\", \"credentials.json\")\n",
    "ENV[\"PARAMETERS\"] = joinpath(pwd(), \"parameters.json\")\n",
    "\n",
    "# Load package\n",
    "using AzureClusterlessHPC\n",
    "batch_clear();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we set up a pool with 1 GPU instance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pool 1 of 1 in southcentralus already exists.\n"
     ]
    }
   ],
   "source": [
    "# Create pool\n",
    "startup_script = \"pool_startup_script_flux.sh\"\n",
    "create_pool_and_resource_file(startup_script);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the required packages and tag packages that we want to use on the batch workers with the `@batchdef` macro:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Random\n",
    "@batchdef using Flux\n",
    "@batchdef using Flux.Optimise: update!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define a simple neural network consisting of 2 dense layers and an activation function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define network\n",
    "model = Chain(\n",
    "  Dense(10, 5, σ),\n",
    "  Dense(5, 2),\n",
    "  softmax);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having defined our model, we implement our loss function. Here we use a simple sum of squares loss:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function\n",
    "@batchdef function loss(model, x, y)\n",
    "    ŷ = model(x)\n",
    "    sum((y .- ŷ).^2)\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create a simple testing data set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training dataset\n",
    "ntrain = 100\n",
    "x = rand(10, ntrain)\n",
    "y = rand(2, ntrain);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we define a function for training our model. The function takes our network, as well as the input and training labels as input arguments and it returns the trained model. Additionally, we create a flag for optionally running the training on a GPU:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training function\n",
    "@batchdef function train_model(model, x, y; cuda=false, learning_rate=1f-3)\n",
    "\n",
    "    # Move to gpu?\n",
    "    if cuda\n",
    "        x |> gpu\n",
    "        y |> gpu\n",
    "        model |> gpu\n",
    "    end\n",
    "\n",
    "    # Optimization\n",
    "    θ = params(model)\n",
    "    opt = Descent(learning_rate)\n",
    "\n",
    "    # Training loop\n",
    "    for j=1:10\n",
    "        print(\"Training epoch $j of 10.\\n\")\n",
    "        \n",
    "        # Evaluate network & compute gradients\n",
    "        grads = gradient(() -> loss(model, x, y), θ)\n",
    "\n",
    "        # Update network weights\n",
    "        for p in θ\n",
    "            update!(opt, p, grads[p])\n",
    "        end\n",
    "    end\n",
    "    return model\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have everything set up, we want to locally test our network and ensure that everything is implemented correctly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch 1 of 10.\n",
      "Training epoch 2 of 10.\n",
      "Training epoch 3 of 10.\n",
      "Training epoch 4 of 10.\n",
      "Training epoch 5 of 10.\n",
      "Training epoch 6 of 10.\n",
      "Training epoch 7 of 10.\n",
      "Training epoch 8 of 10.\n",
      "Training epoch 9 of 10.\n",
      "Training epoch 10 of 10.\n"
     ]
    }
   ],
   "source": [
    "# Test locally\n",
    "model_train = train_model(model, x, y);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have tested our training function locally on a CPU, we can run the actual training on an Azure GPU instance. Instead of manually having to start an instance and moving all required code and data over manually, we can simple execute our training function remotely with `@batchexec`. We set to `cuda` flag to `true` to move the data and network to the GPU of the VM. After training, we copy the trained network back to our notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Monitoring tasks for 'Completed' state, timeout in 60 minutes ...Creating job [FluxDeepLearning_tC2Rx4AY_1]...\n",
      "Uploading file /home/pwitte/.julia/dev/AzureClusterlessHPC/src/runtime/application-cmd to blob container [azureclusterlesstemp]...\n",
      "Uploading file /home/pwitte/.julia/dev/AzureClusterlessHPC/src/runtime/batch_runtime.jl to blob container [azureclusterlesstemp]...\n",
      "Uploading file packages.dat to container [azureclusterlesstemp]...\n",
      "Uploading file task_1.dat to container [azureclusterlesstemp]...\n",
      "....................................................\n",
      "Fetch output from task task_1\n"
     ]
    }
   ],
   "source": [
    "# Train remotely on GPU\n",
    "bctrl = @batchexec train_model(model, x, y; cuda=true)\n",
    "model_train = fetch(bctrl);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also easily run multiple training runs in parallel, e.g. to train with different hyperparameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0.954479 seconds (102.41 k allocations: 6.401 MiB, 14.05% compilation time)\n",
      "Monitoring tasks for 'Completed' state, timeout in 60 minutes ...Creating job [FluxDeepLearning_80TRorha_1]...\n",
      "Uploading file /home/pwitte/.julia/dev/AzureClusterlessHPC/src/runtime/application-cmd to blob container [azureclusterlesstemp]...\n",
      "Uploading file /home/pwitte/.julia/dev/AzureClusterlessHPC/src/runtime/batch_runtime.jl to blob container [azureclusterlesstemp]...\n",
      "Uploading file packages.dat to container [azureclusterlesstemp]...\n",
      "Uploading file task_1.dat to container [azureclusterlesstemp]...\n",
      "Uploading file task_2.dat to container [azureclusterlesstemp]...\n",
      "Uploading file task_3.dat to container [azureclusterlesstemp]...\n",
      "...........................................\n",
      "Fetch output from task task_3..................................................\n",
      "Fetch output from task task_1.......................................................\n",
      "Fetch output from task task_2\n"
     ]
    }
   ],
   "source": [
    "bctrl = @batchexec pmap(α -> train_model(model, x, y; learning_rate=α), [1f-2, 1f-3, 1f-4])\n",
    "models_tune = fetch(bctrl);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last step is the clean up of our resources. We delete the blob container that contains all temporary files and delete the batch pool:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete container and pool specified in the parameter json file\n",
    "delete_container()\n",
    "delete_all_jobs()\n",
    "delete_pool();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Copyright\n",
    "\n",
    "Copyright (c) Microsoft Corporation. All rights reserved.\n",
    "\n",
    "Licensed under the MIT License (MIT). See LICENSE in the repo root for license information."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.6.1",
   "language": "julia",
   "name": "julia-1.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
